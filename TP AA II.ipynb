{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6587MCPn02t1"
      },
      "source": [
        "# Predicción de Puntuación y Capitalización en Texto Normalizado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpd_9mnh1TTw"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from torch import nn\n",
        "\n",
        "RANDOM_SEED = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs9jQujreT2-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCIksjPa2faj"
      },
      "outputs": [],
      "source": [
        "bert_model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "def get_multilingual_token_embedding(token: str):\n",
        "  \"\"\"\n",
        "  Devuelve el embedding (estático) para el token.\n",
        "  \"\"\"\n",
        "  token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "  if token_id is None or token_id == tokenizer.unk_token_id:\n",
        "    print(f\"❌ El token '{token}' no pertenece al vocabulario de multilingual BERT.\")\n",
        "    return None\n",
        "\n",
        "  embedding_vector = bert_model.embeddings.word_embeddings.weight[token_id]\n",
        "\n",
        "  print(f\"✅ Token: '{token}' | ID: {token_id}\")\n",
        "  print(f\"Embedding shape: {embedding_vector.shape}\")\n",
        "  return embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "tokenizer_fast = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "zgTXmh2xFxDy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx1tRuVq4v1y"
      },
      "outputs": [],
      "source": [
        "def capitalizacion_de_palabra(palabra: str) -> int:\n",
        "    if palabra.islower(): return 0\n",
        "    elif palabra.istitle(): return 1\n",
        "    elif palabra.isupper(): return 3\n",
        "    else: return 2\n",
        "\n",
        "def tokenizar_con_etiquetas_list(\n",
        "    oracion: str,\n",
        "    instancia_id: int,\n",
        "    tokenizer: BertTokenizerFast\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Like your old tokenizar_con_etiquetas, but returns a LIST of dicts,\n",
        "    not a tiny DataFrame.  Much cheaper to extend a Python list.\n",
        "    \"\"\"\n",
        "    tokens_reales = re.findall(r\"\\w+['’]?\\w*|¿|¡|\\?|,|\\.|!\", oracion)\n",
        "    rows = []\n",
        "    i = 0\n",
        "    while i < len(tokens_reales):\n",
        "        tok = tokens_reales[i]\n",
        "        # initial‐punct\n",
        "        if tok in INIT_PUNCT_MAP and tok not in ['','.',',','?','!']:\n",
        "            i += 1\n",
        "            if i >= len(tokens_reales):\n",
        "                break\n",
        "            init_lbl = INIT_PUNCT_MAP[tokens_reales[i-1]]\n",
        "            tok = tokens_reales[i]\n",
        "        else:\n",
        "            init_lbl = INIT_PUNCT_MAP['']\n",
        "\n",
        "        # final‐punct\n",
        "        if i+1 < len(tokens_reales) and tokens_reales[i+1] in FINAL_PUNCT_MAP:\n",
        "            final_lbl = FINAL_PUNCT_MAP[tokens_reales[i+1]]\n",
        "            skip_final = True\n",
        "        else:\n",
        "            final_lbl = FINAL_PUNCT_MAP['']\n",
        "            skip_final = False\n",
        "\n",
        "        # BERT sub‑tokens in batch\n",
        "        sub_tokens = tokenizer.tokenize(tok.lower())\n",
        "        cap_lbl = capitalizacion_de_palabra(tok)\n",
        "\n",
        "        for j, sub in enumerate(sub_tokens):\n",
        "            rows.append({\n",
        "                'instancia_id':   instancia_id,\n",
        "                'token_id':       tokenizer.convert_tokens_to_ids(sub),\n",
        "                'token':          sub,\n",
        "                'punt_inicial':   init_lbl   if j==0 else INIT_PUNCT_MAP[''],\n",
        "                'punt_final':     final_lbl  if j==0 else FINAL_PUNCT_MAP[''],\n",
        "                'capitalizacion': cap_lbl\n",
        "            })\n",
        "\n",
        "        i += 1\n",
        "        if skip_final:\n",
        "            i += 1\n",
        "\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg-mqiqP2Yma"
      },
      "outputs": [],
      "source": [
        "def tokenizar_con_etiquetas(\n",
        "    oracion: str,\n",
        "    instancia_id: int,\n",
        "    tokenizer: BertTokenizerFast\n",
        ") -> list[dict]:\n",
        "    \"\"\"\n",
        "    Like your old tokenizar_con_etiquetas, but returns a LIST of dicts.\n",
        "    Much cheaper to extend one big list than to build tiny DataFrames.\n",
        "    \"\"\"\n",
        "    # same INIT_PUNCT_MAP, FINAL_PUNCT_MAP, CAP_MAP, capitalizacion_de_palabra...\n",
        "    tokens_reales = re.findall(r\"\\w+['’]?\\w*|¿|¡|\\?|,|\\.|!\", oracion)\n",
        "    rows = []\n",
        "    i = 0\n",
        "    while i < len(tokens_reales):\n",
        "        tok = tokens_reales[i]\n",
        "        if tok in INIT_PUNCT_MAP and tok not in ['','.',',','?','!']:\n",
        "            i += 1\n",
        "            if i >= len(tokens_reales): break\n",
        "            init_lbl = INIT_PUNCT_MAP[tokens_reales[i-1]]\n",
        "            tok = tokens_reales[i]\n",
        "        else:\n",
        "            init_lbl = INIT_PUNCT_MAP['']\n",
        "\n",
        "        if i+1 < len(tokens_reales) and tokens_reales[i+1] in FINAL_PUNCT_MAP:\n",
        "            final_lbl = FINAL_PUNCT_MAP[tokens_reales[i+1]]\n",
        "            skip_final = True\n",
        "        else:\n",
        "            final_lbl = FINAL_PUNCT_MAP['']\n",
        "            skip_final = False\n",
        "\n",
        "        sub_tokens = tokenizer.tokenize(tok.lower())\n",
        "        cap_lbl = capitalizacion_de_palabra(tok)\n",
        "\n",
        "        for j, sub in enumerate(sub_tokens):\n",
        "            rows.append({\n",
        "                'instancia_id':   instancia_id,\n",
        "                'token_id':       tokenizer.convert_tokens_to_ids(sub),\n",
        "                'token':          sub,\n",
        "                'punt_inicial':   init_lbl   if j==0 else INIT_PUNCT_MAP[''],\n",
        "                'punt_final':     final_lbl  if j==0 else FINAL_PUNCT_MAP[''],\n",
        "                'capitalizacion': cap_lbl\n",
        "            })\n",
        "\n",
        "        i += 1\n",
        "        if skip_final:\n",
        "            i += 1\n",
        "\n",
        "    return rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du2bitp02Ymb"
      },
      "outputs": [],
      "source": [
        "INIT_PUNCT_MAP = {'': 0, '¿': 1}\n",
        "FINAL_PUNCT_MAP = {'': 0, ',': 1, '.': 2, '?': 3}\n",
        "CAP_MAP = {'lower': 0, 'title': 1, 'mixed': 2, 'upper': 3}\n",
        "df = tokenizar_con_etiquetas(\"¿Esperemos a todos los jugadorazos?\", 1, tokenizer_fast)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3IAgE3DCdry"
      },
      "outputs": [],
      "source": [
        "def cargar_dataset(\n",
        "    path: str,\n",
        "    tokenizer: BertTokenizerFast,\n",
        "    n_max_oraciones: int | None = None,\n",
        "    shuffle: bool = False,\n",
        "    random_seed: int = 0,\n",
        "    report_every: int = 500_000\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads up to `n_max_oraciones` lines; optionally shuffles them;\n",
        "    tokenizes/labels in one pass (collecting into a list); logs\n",
        "    progress every `report_every` lines; then builds a single DataFrame.\n",
        "    \"\"\"\n",
        "    # 1) load raw lines\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if n_max_oraciones:\n",
        "            lines = [f.readline() for _ in range(n_max_oraciones)]\n",
        "        else:\n",
        "            lines = f.readlines()\n",
        "\n",
        "    if shuffle:\n",
        "        import random\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(lines)\n",
        "\n",
        "    # 2) process all sentences into one big Python list\n",
        "    all_rows = []\n",
        "    for idx, sent in enumerate(lines, start=1):\n",
        "        all_rows.extend(tokenizar_con_etiquetas(sent, idx, tokenizer))\n",
        "        if idx % report_every == 0:\n",
        "            print(f\"… processed {idx} sentences\")\n",
        "\n",
        "    # 3) one-shot DataFrame construction\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    print(f\"Done: total sentences = {idx}, total tokens = {len(df)}\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QslPUNyXuocN"
      },
      "outputs": [],
      "source": [
        "dataset['punt_final'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9_KS-zQkhcj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "class CapitalizacionDataset(Dataset):\n",
        "    def __init__(self, dataset: pd.DataFrame):\n",
        "        self.dataset = dataset\n",
        "        self.instance_ids = sorted(dataset[\"instancia_id\"].unique())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instance_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        inst_id = self.instance_ids[idx]\n",
        "        row = self.dataset[self.dataset[\"instancia_id\"] == inst_id]\n",
        "\n",
        "        input_ids = torch.tensor(row[\"token_id\"].tolist(), dtype=torch.long)\n",
        "        labels_init = torch.tensor(row[\"punt_inicial\"].tolist(), dtype=torch.long)\n",
        "        labels_final = torch.tensor(row[\"punt_final\"].tolist(), dtype=torch.long)\n",
        "        labels_cap = torch.tensor(row[\"capitalizacion\"].tolist(), dtype=torch.long)\n",
        "\n",
        "        return input_ids, labels_init, labels_final, labels_cap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Z6-TkZol5k"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "PAD_token = 0\n",
        "PAD_token_target = 4\n",
        "MAX_LEN = dataset[\"instancia_id\"].value_counts().max()\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    batch: lista de tuplas (tokens_tensor, target_tensor)\n",
        "    MAX_LEN: longitud fija a la que se debe paddear o truncar\n",
        "    PAD_token: ID usado para el padding\n",
        "    \"\"\"\n",
        "    tokens, targets = [], []\n",
        "    for token_tensor, target_tensor in batch:\n",
        "        tokens.append(token_tensor)\n",
        "        targets.append(target_tensor)\n",
        "\n",
        "    # Primero paddeamos hasta el más largo del batch\n",
        "    tokens_padded = pad_sequence(tokens, batch_first=True, padding_value=PAD_token)\n",
        "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_token_target)\n",
        "\n",
        "    # Luego truncamos o paddeamos a longitud fija MAX_LEN\n",
        "    def pad_or_truncate(tensor, max_len, pad_token):\n",
        "        if tensor.size(1) > max_len:\n",
        "            return tensor[:, :max_len]\n",
        "        elif tensor.size(1) < max_len:\n",
        "            pad_size = max_len - tensor.size(1)\n",
        "            padding = torch.full((tensor.size(0), pad_size), pad_token, dtype=torch.long)\n",
        "            return torch.cat([tensor, padding], dim=1)\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "    tokens_fixed = pad_or_truncate(tokens_padded, MAX_LEN, PAD_token)\n",
        "    targets_fixed = pad_or_truncate(targets_padded, MAX_LEN, PAD_token_target)\n",
        "\n",
        "    return tokens_fixed, targets_fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47f18opM2Ymg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# --- Dataset and Collate Function ---\n",
        "class TokenClassificationDataset(Dataset):\n",
        "    def __init__(self, data_df):\n",
        "        \"\"\"data_df debe contener columnas: instancia_id, token_id, punt_inicial, punt_final, capitalizacion\"\"\"\n",
        "        self.df = data_df\n",
        "        self.instance_ids = sorted(data_df['instancia_id'].unique())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instance_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inst_id = self.instance_ids[idx]\n",
        "        subset = self.df[self.df['instancia_id'] == inst_id]\n",
        "        input_ids = torch.tensor(subset['token_id'].tolist(), dtype=torch.long)\n",
        "        labels_init = torch.tensor(subset['punt_inicial'].tolist(), dtype=torch.long)\n",
        "        labels_final = torch.tensor(subset['punt_final'].tolist(), dtype=torch.long)\n",
        "        labels_cap = torch.tensor(subset['capitalizacion'].tolist(), dtype=torch.long)\n",
        "        return input_ids, labels_init, labels_final, labels_cap\n",
        "\n",
        "PAD_IDX = 0\n",
        "PAD_LABEL_INIT = 0  # ajustar según mapeo de etiquetas\n",
        "PAD_LABEL_FINAL = 0\n",
        "PAD_LABEL_CAP = 0\n",
        "MAX_LEN = 128  # o el que convenga\n",
        "\n",
        "def collate_batch(batch):\n",
        "    inputs, l_init, l_final, l_cap = zip(*batch)\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
        "    init_padded = pad_sequence(l_init, batch_first=True, padding_value=PAD_LABEL_INIT)\n",
        "    final_padded = pad_sequence(l_final, batch_first=True, padding_value=PAD_LABEL_FINAL)\n",
        "    cap_padded = pad_sequence(l_cap, batch_first=True, padding_value=PAD_LABEL_CAP)\n",
        "    # truncar o pad a MAX_LEN\n",
        "    if inputs_padded.size(1) > MAX_LEN:\n",
        "        inputs_padded = inputs_padded[:, :MAX_LEN]\n",
        "        init_padded = init_padded[:, :MAX_LEN]\n",
        "        final_padded = final_padded[:, :MAX_LEN]\n",
        "        cap_padded = cap_padded[:, :MAX_LEN]\n",
        "    else:\n",
        "        pad_size = MAX_LEN - inputs_padded.size(1)\n",
        "        inputs_padded = nn.functional.pad(inputs_padded, (0, pad_size), value=PAD_IDX)\n",
        "        init_padded = nn.functional.pad(init_padded, (0, pad_size), value=PAD_LABEL_INIT)\n",
        "        final_padded = nn.functional.pad(final_padded, (0, pad_size), value=PAD_LABEL_FINAL)\n",
        "        cap_padded = nn.functional.pad(cap_padded, (0, pad_size), value=PAD_LABEL_CAP)\n",
        "    return inputs_padded, init_padded, final_padded, cap_padded\n",
        "\n",
        "# --- Model ---\n",
        "class BiLSTMMultiHead(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_model_name: str,\n",
        "                 hid_dim: int,\n",
        "                 n_init: int,\n",
        "                 n_final: int,\n",
        "                 n_cap: int,\n",
        "                 n_layers: int = 1,\n",
        "                 dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # BERT embeddings as input\n",
        "        bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.embedding = bert.embeddings.word_embeddings\n",
        "        emb_dim = bert.config.hidden_size\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=hid_dim,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if n_layers > 1 else 0.0,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc_init = nn.Linear(hid_dim * 2, n_init)\n",
        "        self.fc_final = nn.Linear(hid_dim * 2, n_final)\n",
        "        self.fc_cap = nn.Linear(hid_dim * 2, n_cap)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids: [batch, seq]\n",
        "        x = self.embedding(input_ids)  # [batch, seq, emb_dim]\n",
        "        x, _ = self.lstm(x)            # [batch, seq, hid_dim*2]\n",
        "        x = self.dropout(x)\n",
        "        out_init = self.fc_init(x)     # [batch, seq, n_init]\n",
        "        out_final = self.fc_final(x)   # [batch, seq, n_final]\n",
        "        out_cap = self.fc_cap(x)       # [batch, seq, n_cap]\n",
        "        return out_init, out_final, out_cap\n",
        "\n",
        "# --- Training Loop ---\n",
        "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, init_lbl, final_lbl, cap_lbl in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        init_lbl = init_lbl.to(device)\n",
        "        final_lbl = final_lbl.to(device)\n",
        "        cap_lbl = cap_lbl.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_init, logits_final, logits_cap = model(inputs)\n",
        "\n",
        "        # reshape for Loss: (batch*seq, classes)\n",
        "        bs, seq_len, _ = logits_init.size()\n",
        "        loss_init = criterion(logits_init.view(-1, logits_init.size(-1)), init_lbl.view(-1))\n",
        "        loss_final = criterion(logits_final.view(-1, logits_final.size(-1)), final_lbl.view(-1))\n",
        "        loss_cap = criterion(logits_cap.view(-1, logits_cap.size(-1)), cap_lbl.view(-1))\n",
        "        loss = loss_init + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# --- Evaluation ---\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, init_lbl, final_lbl, cap_lbl in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            init_lbl = init_lbl.to(device)\n",
        "            final_lbl = final_lbl.to(device)\n",
        "            cap_lbl = cap_lbl.to(device)\n",
        "\n",
        "            logits_init, logits_final, logits_cap = model(inputs)\n",
        "            loss_init = criterion(logits_init.view(-1, logits_init.size(-1)), init_lbl.view(-1))\n",
        "            loss_final = criterion(logits_final.view(-1, logits_final.size(-1)), final_lbl.view(-1))\n",
        "            loss_cap = criterion(logits_cap.view(-1, logits_cap.size(-1)), cap_lbl.view(-1))\n",
        "            total_loss += (loss_init + loss_final + loss_cap).item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "def predict_sentence(model, tokenizer: BertTokenizer, sentence: str, device):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits_init, logits_final, logits_cap = model(input_ids)\n",
        "    preds_init = torch.argmax(logits_init, dim=-1).squeeze(0).cpu().tolist()\n",
        "    preds_final = torch.argmax(logits_final, dim=-1).squeeze(0).cpu().tolist()\n",
        "    preds_cap = torch.argmax(logits_cap, dim=-1).squeeze(0).cpu().tolist()\n",
        "    return list(zip(tokens, preds_init, preds_final, preds_cap))\n",
        "\n",
        "# --- Ejemplo de uso ---\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "# model = BiLSTMMultiHead(\n",
        "#     bert_model_name='bert-base-multilingual-cased',\n",
        "#     hid_dim=256,\n",
        "#     n_init=5,\n",
        "#     n_final=5,\n",
        "#     n_cap=4,\n",
        "#     n_layers=2,\n",
        "#     dropout=0.2\n",
        "# ).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=PAD_LABEL_INIT)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_batch)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_batch)\n",
        "# for epoch in range(10):\n",
        "#     train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "#     val_loss = evaluate(model, val_loader, criterion, device)\n",
        "#     print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UaR69Op2Ymh"
      },
      "outputs": [],
      "source": [
        "def restore_text(\n",
        "    model,\n",
        "    tokenizer: BertTokenizer,\n",
        "    sentence: str,\n",
        "    device: torch.device,\n",
        "    init_map: dict,\n",
        "    final_map: dict,\n",
        "    cap_map: dict\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Reconstructs a sentence by applying token-level predictions:\n",
        "      - init_map:    {label_idx: init_punct_str, …}\n",
        "      - final_map:   {label_idx: final_punct_str, …}\n",
        "      - cap_map:     {label_idx: 'lower'|'title'|'mixed'|'upper', …}\n",
        "\n",
        "    Returns the fully punctuated & capitalized sentence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # 1) tokenize & convert to IDs\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2) model forward → get label logits\n",
        "    with torch.no_grad():\n",
        "        logits_init, logits_final, logits_cap = model(input_ids)\n",
        "\n",
        "    # 3) take argmax to get class indices\n",
        "    preds_init  = torch.argmax(logits_init,  dim=-1).squeeze(0).cpu().tolist()\n",
        "    preds_final = torch.argmax(logits_final, dim=-1).squeeze(0).cpu().tolist()\n",
        "    preds_cap   = torch.argmax(logits_cap,   dim=-1).squeeze(0).cpu().tolist()\n",
        "    print(tokens)\n",
        "    print(preds_init)\n",
        "    print(preds_final)\n",
        "    print(preds_cap)\n",
        "\n",
        "    # 4) rebuild words, merging wordpieces\n",
        "    words: list[str] = []\n",
        "    for token, i_init, i_fin, i_cap in zip(tokens, preds_init, preds_final, preds_cap):\n",
        "        piece = token\n",
        "        if piece.startswith(\"##\"):\n",
        "            # merge onto previous word\n",
        "            words[-1] += piece[2:]\n",
        "            continue\n",
        "\n",
        "        # apply capitalization\n",
        "        cap_label = cap_map[i_cap]\n",
        "        if cap_label == \"upper\":\n",
        "            piece = piece.upper()\n",
        "        elif cap_label == \"title\":\n",
        "            piece = piece.capitalize()\n",
        "        # lower or mixed: keep as-is (mixed will contain original casing in training)\n",
        "\n",
        "        # prepend initial punct, append final punct\n",
        "        init_sign = init_map.get(i_init, \"\")\n",
        "        fin_sign  = final_map.get(i_fin, \"\")\n",
        "\n",
        "        words.append(f\"{init_sign}{piece}{fin_sign}\")\n",
        "\n",
        "    # 5) join & clean up spaces before punctuation\n",
        "    sentence_restored = \" \".join(words)\n",
        "    for p in [\",\", \".\", \"?\", \"!\", \":\", \";\"]:\n",
        "        sentence_restored = sentence_restored.replace(f\" {p}\", p)\n",
        "\n",
        "    return sentence_restored\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQK4_BCm9J3g"
      },
      "outputs": [],
      "source": [
        "data_path = \"es_419_validas.txt\"\n",
        "dataset = cargar_dataset(data_path, random_seed=RANDOM_SEED,tokenizer = tokenizer_fast,n_max_oraciones=100000)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ1Iuscv2Ymh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1) Prepare dataset + split\n",
        "full_dataset = CapitalizacionDataset(dataset)  # your DataFrame → Dataset\n",
        "train_size = int(len(full_dataset) * 0.9)\n",
        "val_size   = len(full_dataset) - train_size\n",
        "train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "\n",
        "# 2) Instantiate model, tokenizer, device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = tokenizer_fast\n",
        "\n",
        "model = BiLSTMMultiHead(\n",
        "    bert_model_name=\"bert-base-multilingual-cased\",\n",
        "    hid_dim=256,\n",
        "    n_init=len(INIT_PUNCT_MAP),\n",
        "    n_final=len(FINAL_PUNCT_MAP),\n",
        "    n_cap=len(CAP_MAP),\n",
        "    n_layers=3,\n",
        "    dropout=0.1,\n",
        ").to(device)\n",
        "\n",
        "# 3) Loss + optimizer\n",
        "# We’ll ignore pad‐labels for all three heads\n",
        "ignore_idx = PAD_LABEL_INIT  # make sure PAD_LABEL_INIT == PAD_LABEL_FINAL == PAD_LABEL_CAP\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=ignore_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# 4) Training loop\n",
        "n_epochs = 3\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss   = evaluate(  model, val_loader,   criterion, device)\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}] \"\n",
        "          f\"Train Loss = {train_loss:.4f}  |  Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "# 5) Save your model\n",
        "torch.save(model.state_dict(), \"bilstm_multihead.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0MKNCcr2Ymh"
      },
      "outputs": [],
      "source": [
        "# invert your maps:\n",
        "init_map  = {v:k for k,v in INIT_PUNCT_MAP.items()}\n",
        "final_map = {v:k for k,v in FINAL_PUNCT_MAP.items()}\n",
        "cap_map   = {v:k for k,v in CAP_MAP.items()}\n",
        "\n",
        "output = restore_text(\n",
        "    model, tokenizer,\n",
        "     \"estoy podrido de esto por que esta pasando ayuda por favor me estoy muriendo\",\n",
        "    device,\n",
        "    init_map=init_map,\n",
        "    final_map=final_map,\n",
        "    cap_map=cap_map\n",
        ")\n",
        "print(output)  # → \"arquitectura\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(init_map)\n",
        "print(final_map)\n",
        "print(cap_map)"
      ],
      "metadata": {
        "id": "KXNzlj8hVgKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_map"
      ],
      "metadata": {
        "id": "uyRennDB4chN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "Ichx8S_XJbuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from torch import nn\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Lowercases and removes punctuation from the text, returning a \"clean\" version.\n",
        "    \"\"\"\n",
        "    # remove leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation (keep letters, numbers and spaces)\n",
        "    text = re.sub(r\"[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def cargar_dataset(\n",
        "    path: str,\n",
        "    tokenizer: BertTokenizerFast,\n",
        "    n_max_oraciones: int | None = None,\n",
        "    shuffle: bool = False,\n",
        "    random_seed: int = 0,\n",
        "    report_every: int = 500_000\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads up to `n_max_oraciones` lines; optionally shuffles them;\n",
        "    tokenizes/labels in one pass (collecting into a list); logs\n",
        "    progress every `report_every` lines; then returns a DataFrame\n",
        "    with two columns:\n",
        "      - raw_sentence: original sentence (with punctuation & casing)\n",
        "      - clean_sentence: lowercase, punctuation-free version\n",
        "    \"\"\"\n",
        "    # 1) load raw lines\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if n_max_oraciones:\n",
        "            raw_lines = [f.readline().rstrip(\"\\n\") for _ in range(n_max_oraciones)]\n",
        "        else:\n",
        "            raw_lines = [line.rstrip(\"\\n\") for line in f]\n",
        "\n",
        "    # 2) optional shuffle\n",
        "    if shuffle:\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(raw_lines)\n",
        "\n",
        "    # 3) build DataFrame rows\n",
        "    rows = []\n",
        "    for idx, raw in enumerate(raw_lines, start=1):\n",
        "        clean = clean_text(raw)\n",
        "        rows.append({\n",
        "            'raw_sentence': raw,\n",
        "            'clean_sentence': clean\n",
        "        })\n",
        "        if idx % report_every == 0:\n",
        "            print(f\"… processed {idx} sentences\")\n",
        "\n",
        "    # 4) assemble DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "    print(f\"Done: total sentences = {len(df)}\")\n",
        "    return df\n",
        "\n",
        "df = cargar_dataset(\"es_419_validas.txt\", tokenizer_fast, n_max_oraciones=150000)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "xEftenc7LWAx",
        "outputId": "35040c7e-0260-46bc-9d61-20c52d823ee1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: total sentences = 150000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           raw_sentence                       clean_sentence\n",
              "0           Te mostraré los resultados.           te mostraré los resultados\n",
              "1    Me permite hablar por los muertos.    me permite hablar por los muertos\n",
              "2            Serás Margaret Penobscott.            serás margaret penobscott\n",
              "3  Somos compañeras de clase de Mariko.  somos compañeras de clase de mariko\n",
              "4                        Pasado mañana.                        pasado mañana"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57ec0b6c-17d3-46c6-afd6-b4bada78c10b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_sentence</th>\n",
              "      <th>clean_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Te mostraré los resultados.</td>\n",
              "      <td>te mostraré los resultados</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Me permite hablar por los muertos.</td>\n",
              "      <td>me permite hablar por los muertos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Serás Margaret Penobscott.</td>\n",
              "      <td>serás margaret penobscott</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Somos compañeras de clase de Mariko.</td>\n",
              "      <td>somos compañeras de clase de mariko</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pasado mañana.</td>\n",
              "      <td>pasado mañana</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57ec0b6c-17d3-46c6-afd6-b4bada78c10b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57ec0b6c-17d3-46c6-afd6-b4bada78c10b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57ec0b6c-17d3-46c6-afd6-b4bada78c10b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-422b432f-3c43-46e2-bb6f-33318d7331fc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-422b432f-3c43-46e2-bb6f-33318d7331fc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-422b432f-3c43-46e2-bb6f-33318d7331fc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "# import argparse # Remove argparse as it's not needed in the notebook context\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 d_model: int = 256,\n",
        "                 nhead: int = 4,\n",
        "                 num_encoder_layers: int = 3,\n",
        "                 num_decoder_layers: int = 3,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1,\n",
        "                 pad_token_id: int = 0,\n",
        "                 max_seq_len: int = 128):\n",
        "        super().__init__()\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
        "        self.positional_encoding = nn.Parameter(\n",
        "            torch.zeros(max_seq_len, d_model), requires_grad=True\n",
        "        )\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self,\n",
        "                src_ids: torch.LongTensor,\n",
        "                tgt_ids: torch.LongTensor,\n",
        "                src_key_padding_mask: torch.BoolTensor = None,\n",
        "                tgt_key_padding_mask: torch.BoolTensor = None,\n",
        "                tgt_mask: torch.Tensor = None):\n",
        "        # Embedding + positional\n",
        "        seq_len_src = src_ids.size(1)\n",
        "        seq_len_tgt = tgt_ids.size(1)\n",
        "        src_emb = self.embedding(src_ids) + self.positional_encoding[:seq_len_src]\n",
        "        tgt_emb = self.embedding(tgt_ids) + self.positional_encoding[:seq_len_tgt]\n",
        "\n",
        "        # Transformer\n",
        "        out = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask\n",
        "        )\n",
        "        # Final projection\n",
        "        return self.output_layer(out)\n",
        "\n",
        "class RestorationDataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer, max_len=64):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.examples[idx][0]\n",
        "        tgt = self.examples[idx][1]\n",
        "        # Encode source\n",
        "        src_enc = self.tokenizer.encode_plus(\n",
        "            src,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # Encode target\n",
        "        tgt_enc = self.tokenizer.encode_plus(\n",
        "            tgt,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        src_ids = src_enc.input_ids.squeeze(0)\n",
        "        src_mask = src_enc.attention_mask.squeeze(0).bool()\n",
        "        full_tgt_ids = tgt_enc.input_ids.squeeze(0)\n",
        "        # decoder inputs and labels\n",
        "        decoder_input_ids = full_tgt_ids[:-1]\n",
        "        decoder_attention_mask = tgt_enc.attention_mask.squeeze(0)[:-1].bool()\n",
        "        labels = full_tgt_ids[1:].clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'src_ids': src_ids,\n",
        "            'src_mask': ~src_mask,\n",
        "            'tgt_ids': decoder_input_ids,\n",
        "            'tgt_mask': ~decoder_attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
        "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
        "    return mask\n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        src_ids = batch['src_ids'].to(device)\n",
        "        tgt_ids = batch['tgt_ids'].to(device)\n",
        "        src_key_mask = batch['src_mask'].to(device)\n",
        "        tgt_key_mask = batch['tgt_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt_ids.size(1)).to(device)\n",
        "        outputs = model(\n",
        "            src_ids=src_ids,\n",
        "            tgt_ids=tgt_ids,\n",
        "            src_key_padding_mask=src_key_mask,\n",
        "            tgt_key_padding_mask=tgt_key_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        # outputs shape: (B, T, V)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src_ids = batch['src_ids'].to(device)\n",
        "            tgt_ids = batch['tgt_ids'].to(device)\n",
        "            src_key_mask = batch['src_mask'].to(device)\n",
        "            tgt_key_mask = batch['tgt_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt_ids.size(1)).to(device)\n",
        "            outputs = model(\n",
        "                src_ids=src_ids,\n",
        "                tgt_ids=tgt_ids,\n",
        "                src_key_padding_mask=src_key_mask,\n",
        "                tgt_key_padding_mask=tgt_key_mask,\n",
        "                tgt_mask=tgt_mask\n",
        "            )\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def greedy_decode(model, tokenizer, src_sentence: str, device, max_len: int = 64):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_enc = tokenizer.encode_plus(\n",
        "            src_sentence,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        src_ids = src_enc.input_ids.to(device)\n",
        "        # Ensure src_mask is 2D by unsqueezing the batch dimension if it's 1D\n",
        "        src_mask = ~src_enc.attention_mask.bool().to(device)\n",
        "        if src_mask.dim() == 1:\n",
        "            src_mask = src_mask.unsqueeze(0)\n",
        "\n",
        "        # Encode memory\n",
        "        seq_len_src = src_ids.size(1)\n",
        "        memory = model.embedding(src_ids) + model.positional_encoding[:seq_len_src]\n",
        "        memory = model.transformer.encoder(memory, src_key_padding_mask=src_mask)\n",
        "\n",
        "        ys = torch.full((1, 1), tokenizer.cls_token_id, dtype=torch.long).to(device)\n",
        "        for i in range(max_len - 1):\n",
        "            tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(device)\n",
        "            tgt_emb = model.embedding(ys) + model.positional_encoding[:ys.size(1)]\n",
        "            out = model.transformer.decoder(\n",
        "                tgt=tgt_emb,\n",
        "                memory=memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_key_padding_mask=src_mask # memory_key_padding_mask should also be 2D\n",
        "            )\n",
        "            logits = model.output_layer(out[:, -1, :])\n",
        "            next_token = logits.argmax(dim=-1).unsqueeze(1)\n",
        "            ys = torch.cat([ys, next_token], dim=1)\n",
        "            if next_token.item() == tokenizer.sep_token_id:\n",
        "                break\n",
        "        return tokenizer.decode(ys.squeeze(), skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Remove the if __name__ == '__main__': block and argparse calls\n",
        "# Replace it with direct argument setting and function call\n",
        "class Args: # Simple class to mimic argparse Namespace\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 8\n",
        "    lr: float = 5e-4\n",
        "    max_len: int = 64\n",
        "    save_path: str = 'best_model.pt'\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Call the main function with the defined arguments\n",
        "main(args) # Commenting out the main function call to avoid running it automatically"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLdsoqPIJdT2",
        "outputId": "e6ac3f5d-2058-459b-f228-9a8fd09d663b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 11.6433 | Val Loss: 10.0844\n",
            "Epoch 2 | Train Loss: 10.0698 | Val Loss: 9.0064\n",
            "Epoch 3 | Train Loss: 9.2176 | Val Loss: 8.2967\n",
            "Epoch 4 | Train Loss: 8.8104 | Val Loss: 7.7722\n",
            "Epoch 5 | Train Loss: 8.3723 | Val Loss: 7.3332\n",
            "Input:    hola como estas\n",
            "Restored: ##lalalalala Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como Como\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(epochs,batch_size,lr,max_len,save_path):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "    df = cargar_dataset(\"es_419_validas.txt\", tokenizer_fast, n_max_oraciones=150000)\n",
        "    dataset = RestorationDataset(df, tokenizer, max_len=args.max_len)\n",
        "    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(dataset, batch_size=args.batch_size)\n",
        "\n",
        "    model = TransformerAutoencoder(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        max_seq_len=args.max_len\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    best_val = float('inf')\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss = evaluate(model, val_loader, criterion, device)\n",
        "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), args.save_path)\n",
        "\n",
        "    # Demo inference\n",
        "    test_sent = \"hola como estas\"\n",
        "    print(\"Input:   \", test_sent)\n",
        "    print(\"Restored:\", greedy_decode(model, tokenizer, test_sent, device, args.max_len))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(3,32,5e-4,64,\"transformer_autoencoder.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "h4PbDJhvMJqp",
        "outputId": "50f83da4-eedf-45a5-dda6-b1620b4e22f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: total sentences = 150000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "30565",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 30565",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-48058570.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"transformer_autoencoder.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-19-48058570.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(epochs, batch_size, lr, max_len, save_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-1561793033.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0msrc_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-1561793033.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Encode source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 30565"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}