{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22Um16b8hYaa"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries (run in the notebook environment)\n",
        "!pip install torch transformers tqdm pandas scikit-learn\n",
        "\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-mvov0XojBH"
      },
      "source": [
        "# Pre procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H_cZspapu_X",
        "outputId": "1ffe82c5-d584-4853-a386-83b88d4410d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "… loaded 100000 sentences\n",
            "… loaded 200000 sentences\n",
            "… loaded 300000 sentences\n",
            "… loaded 400000 sentences\n",
            "… loaded 500000 sentences\n",
            "… loaded 600000 sentences\n",
            "… loaded 700000 sentences\n",
            "… loaded 800000 sentences\n",
            "… loaded 900000 sentences\n",
            "… loaded 1000000 sentences\n",
            "Done loading: 1000000 sentences\n",
            "Shuffled sentences\n",
            "… processed 50000 sentences, 410378 tokens so far\n",
            "… processed 100000 sentences, 818329 tokens so far\n",
            "… processed 150000 sentences, 1227981 tokens so far\n",
            "… processed 200000 sentences, 1637014 tokens so far\n",
            "… processed 250000 sentences, 2047872 tokens so far\n",
            "… processed 300000 sentences, 2454083 tokens so far\n",
            "… processed 350000 sentences, 2865926 tokens so far\n",
            "… processed 400000 sentences, 3276045 tokens so far\n",
            "… processed 450000 sentences, 3686057 tokens so far\n",
            "… processed 500000 sentences, 4094624 tokens so far\n",
            "… processed 550000 sentences, 4502102 tokens so far\n",
            "… processed 600000 sentences, 4912328 tokens so far\n",
            "… processed 650000 sentences, 5321509 tokens so far\n",
            "… processed 700000 sentences, 5728622 tokens so far\n",
            "… processed 750000 sentences, 6137951 tokens so far\n",
            "… processed 800000 sentences, 6548009 tokens so far\n",
            "… processed 850000 sentences, 6957975 tokens so far\n",
            "… processed 900000 sentences, 7368115 tokens so far\n",
            "… processed 950000 sentences, 7777391 tokens so far\n",
            "… processed 1000000 sentences, 8188286 tokens so far\n",
            "Final: 8188286 tokens from 1000000 sentences\n",
            "   inst_id  token_id  token punt_inicial punt_final  capitalizacion\n",
            "0        1         0     no                                       1\n",
            "1        1         1  tanto                                       0\n",
            "2        1         2   como                                       0\n",
            "3        1         3     te                                       0\n",
            "4        1         4    qui                                       0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from typing import List, Optional\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 1) Loader function\n",
        "def read_raw_sentences(\n",
        "    path: str,\n",
        "    n_max_sentences: Optional[int] = None,\n",
        "    shuffle: bool = False,\n",
        "    random_seed: int = 0,\n",
        "    report_every: int = 100_000\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Reads up to `n_max_sentences` lines from `path` (one sentence per line).\n",
        "    Returns a list of the raw lines (with trailing newlines stripped).\n",
        "    \"\"\"\n",
        "    sentences: List[str] = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if n_max_sentences is not None:\n",
        "            for i in range(n_max_sentences):\n",
        "                line = f.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                sentences.append(line.rstrip(\"\\n\"))\n",
        "                if (i + 1) % report_every == 0:\n",
        "                    print(f\"… loaded {i+1} sentences\")\n",
        "        else:\n",
        "            for i, line in enumerate(f, start=1):\n",
        "                sentences.append(line.rstrip(\"\\n\"))\n",
        "                if i % report_every == 0:\n",
        "                    print(f\"… loaded {i} sentences\")\n",
        "    print(f\"Done loading: {len(sentences)} sentences\")\n",
        "    if shuffle:\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(sentences)\n",
        "        print(\"Shuffled sentences\")\n",
        "    return sentences\n",
        "\n",
        "# 2) Your label extractor & pattern\n",
        "pattern = re.compile(r\"\\w+|[^\\w\\s]\", flags=re.UNICODE)\n",
        "\n",
        "def extract_labels(sentence: str):\n",
        "    tokens = pattern.findall(sentence)\n",
        "    words, init_labels, final_labels, cap_labels = [], [], [], []\n",
        "    for i, token in enumerate(tokens):\n",
        "        if re.match(r\"\\w+\", token, flags=re.UNICODE):\n",
        "            # initial punctuation\n",
        "            init = '¿' if i>0 and tokens[i-1]=='¿' else ''\n",
        "            # final punctuation\n",
        "            final = tokens[i+1] if i < len(tokens)-1 and tokens[i+1] in {'.',',','?'} else ''\n",
        "            # capitalization\n",
        "            if token.isupper():\n",
        "                cap = 3\n",
        "            elif token[0].isupper() and token[1:].islower():\n",
        "                cap = 1\n",
        "            elif token.islower():\n",
        "                cap = 0\n",
        "            else:\n",
        "                cap = 2\n",
        "            words.append(token)\n",
        "            init_labels.append(init)\n",
        "            final_labels.append(final)\n",
        "            cap_labels.append(cap)\n",
        "    return words, init_labels, final_labels, cap_labels\n",
        "\n",
        "# 3) Load sentences from file\n",
        "path = \"es_419_validas.txt\"\n",
        "raw_sentences = read_raw_sentences(\n",
        "    path=path,\n",
        "    n_max_sentences=1000000,   # or None to load all\n",
        "    shuffle=True,\n",
        "    random_seed=42,\n",
        "    report_every=100000\n",
        ")\n",
        "\n",
        "# 4) Tokenize+label into DataFrame\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "data = []\n",
        "for inst_id, sentence in enumerate(raw_sentences, start=1):\n",
        "    words, init_lbls, final_lbls, cap_lbls = extract_labels(sentence)\n",
        "    token_idx = 0\n",
        "    for word, init_lbl, final_lbl, cap_lbl in zip(words, init_lbls, final_lbls, cap_lbls):\n",
        "        subtokens = tokenizer.tokenize(word.lower())\n",
        "        for i, sub in enumerate(subtokens):\n",
        "            # initial only on first subtoken\n",
        "            punct_init = init_lbl if i == 0 else ''\n",
        "            # final only on last subtoken\n",
        "            punct_final = final_lbl if i == len(subtokens)-1 else ''\n",
        "            data.append([\n",
        "                inst_id,\n",
        "                token_idx,\n",
        "                sub,\n",
        "                punct_init,\n",
        "                punct_final,\n",
        "                cap_lbl\n",
        "            ])\n",
        "            token_idx += 1\n",
        "    if inst_id % 50_000 == 0:\n",
        "        print(f\"… processed {inst_id} sentences, {len(data)} tokens so far\")\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    data,\n",
        "    columns=[\"inst_id\", \"token_id\", \"token\", \"punt_inicial\", \"punt_final\", \"capitalizacion\"]\n",
        ")\n",
        "print(f\"Final: {df.shape[0]} tokens from {inst_id} sentences\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QacLF4AJoMf3",
        "outputId": "e1c6adbf-1c3f-413e-f036-37fdc12cf513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train instances: 800000, Val: 100000, Test: 100000\n"
          ]
        }
      ],
      "source": [
        "# Convert token strings to BERT token IDs\n",
        "df[\"token_id_bert\"] = tokenizer.convert_tokens_to_ids(df[\"token\"].tolist())\n",
        "\n",
        "# Group by instance to form sequences\n",
        "grouped = {}\n",
        "for inst_id, group in df.groupby(\"inst_id\"):\n",
        "    grouped[inst_id] = {\n",
        "        \"input_ids\": group[\"token_id_bert\"].tolist(),\n",
        "        \"init_labels\": [0 if lbl=='' else 1 for lbl in group[\"punt_inicial\"]],\n",
        "        \"final_labels\": [0 if lbl=='' else (1 if lbl=='.' else (2 if lbl=='?' else 3))\n",
        "                         for lbl in group[\"punt_final\"]],\n",
        "        \"cap_labels\": group[\"capitalizacion\"].tolist(),\n",
        "        \"tokens\": group[\"token\"].tolist()\n",
        "    }\n",
        "\n",
        "# Create a list of instances for splitting\n",
        "instances = list(grouped.values())\n",
        "random.shuffle(instances)\n",
        "n = len(instances)\n",
        "train_split = int(0.8 * n)\n",
        "val_split = int(0.9 * n)\n",
        "train_data = instances[:train_split]\n",
        "val_data   = instances[train_split:val_split]\n",
        "test_data  = instances[val_split:]\n",
        "\n",
        "print(f\"Train instances: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlUbbNeCoO2n"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class PunctCapitalDataset(Dataset):\n",
        "    def __init__(self, instances):\n",
        "        self.instances = instances\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "    def __getitem__(self, idx):\n",
        "        inst = self.instances[idx]\n",
        "        return (\n",
        "            torch.tensor(inst[\"input_ids\"], dtype=torch.long),\n",
        "            torch.tensor(inst[\"init_labels\"], dtype=torch.long),\n",
        "            torch.tensor(inst[\"final_labels\"], dtype=torch.long),\n",
        "            torch.tensor(inst[\"cap_labels\"], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids, init_labs, final_labs, cap_labs = zip(*batch)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    init_labs  = pad_sequence(init_labs,  batch_first=True, padding_value=-100)\n",
        "    final_labs = pad_sequence(final_labs, batch_first=True, padding_value=-100)\n",
        "    cap_labs   = pad_sequence(cap_labs,   batch_first=True, padding_value=-100)\n",
        "    return input_ids, init_labs, final_labs, cap_labs\n",
        "\n",
        "train_loader = DataLoader(PunctCapitalDataset(train_data), batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(PunctCapitalDataset(val_data), batch_size=128, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7920b_Xogs_"
      },
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL0YtW0koTiA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class JointPunctCapitalModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        hidden_dim: int,\n",
        "        num_init: int,\n",
        "        num_final: int,\n",
        "        num_cap: int,\n",
        "        n_layers: int = 1,\n",
        "        dropout: float = 0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Embedding + input dropout\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # BiLSTM with inter-layer dropout (only applies if n_layers > 1)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim // 2,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout if n_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Output dropout before heads\n",
        "        self.output_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Three classification heads\n",
        "        self.init_head  = nn.Linear(hidden_dim, num_init)\n",
        "        self.final_head = nn.Linear(hidden_dim, num_final)\n",
        "        self.cap_head   = nn.Linear(hidden_dim, num_cap)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        emb = self.embedding(x)          # [B, T, E]\n",
        "        emb = self.input_dropout(emb)    # dropout on embeddings\n",
        "\n",
        "        out, _ = self.bilstm(emb)        # [B, T, H]\n",
        "        out = self.output_dropout(out)   # dropout on LSTM outputs\n",
        "\n",
        "        init_logits  = self.init_head(out)    # [B, T, num_init]\n",
        "        final_logits = self.final_head(out)   # [B, T, num_final]\n",
        "        cap_logits   = self.cap_head(out)     # [B, T, num_cap]\n",
        "        return init_logits, final_logits, cap_logits\n",
        "\n",
        "# Example instantiation:\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = JointPunctCapitalModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_init=2,\n",
        "    num_final=4,\n",
        "    num_cap=4,\n",
        "    n_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66stgr7oeOo"
      },
      "source": [
        "# Entrenamiento y evaluacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqBqZlrQoYJv",
        "outputId": "53aa6d58-e8c6-446b-95ca-fe07b862668f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 — Train loss: 0.3058\n",
            "Epoch 1 — Val loss:   0.2232\n",
            "Epoch 1 — F1 (macro): init=0.859, final=0.831, cap=0.924\n",
            "\n",
            "Initial punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        no-¿       0.99      1.00      0.99    799066\n",
            "           ¿       0.89      0.61      0.72     19033\n",
            "\n",
            "    accuracy                           0.99    818099\n",
            "   macro avg       0.94      0.80      0.86    818099\n",
            "weighted avg       0.99      0.99      0.99    818099\n",
            "\n",
            "Final punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        none       0.98      0.99      0.98    687912\n",
            "           .       0.85      0.97      0.90     76049\n",
            "           ?       0.90      0.64      0.75     19369\n",
            "           ,       0.83      0.59      0.69     34769\n",
            "\n",
            "    accuracy                           0.96    818099\n",
            "   macro avg       0.89      0.80      0.83    818099\n",
            "weighted avg       0.96      0.96      0.96    818099\n",
            "\n",
            "Capitalization per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       lower       0.98      0.99      0.99    637060\n",
            "     Initial       0.97      0.94      0.95    172220\n",
            "       Mixed       0.95      0.78      0.86      2949\n",
            "      ALLCAP       0.95      0.85      0.90      5870\n",
            "\n",
            "    accuracy                           0.98    818099\n",
            "   macro avg       0.96      0.89      0.92    818099\n",
            "weighted avg       0.98      0.98      0.98    818099\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "for epoch in range(1, 1+3):  # e.g. 5 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "    for input_ids, init_labs, final_labs, cap_labs in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        init_labs  = init_labs.to(device)\n",
        "        final_labs = final_labs.to(device)\n",
        "        cap_labs   = cap_labs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        init_logits, final_logits, cap_logits = model(input_ids)\n",
        "\n",
        "        loss_init  = criterion(init_logits.view(-1, 2),  init_labs.view(-1))\n",
        "        loss_final = criterion(final_logits.view(-1, 4), final_labs.view(-1))\n",
        "        loss_cap   = criterion(cap_logits.view(-1, 4),   cap_labs.view(-1))\n",
        "        loss = loss_init + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    avg_train_loss = running_loss / n_batches\n",
        "    print(f\"Epoch {epoch} — Train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    n_val_batches = 0\n",
        "\n",
        "    all_init_trues,  all_init_preds  = [], []\n",
        "    all_final_trues, all_final_preds = [], []\n",
        "    all_cap_trues,   all_cap_preds   = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, init_labs, final_labs, cap_labs in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            init_labs  = init_labs.to(device)\n",
        "            final_labs = final_labs.to(device)\n",
        "            cap_labs   = cap_labs.to(device)\n",
        "\n",
        "            init_logits, final_logits, cap_logits = model(input_ids)\n",
        "\n",
        "            # compute val loss\n",
        "            loss_init  = criterion(init_logits.view(-1, 2),  init_labs.view(-1))\n",
        "            loss_final = criterion(final_logits.view(-1, 4), final_labs.view(-1))\n",
        "            loss_cap   = criterion(cap_logits.view(-1, 4),   cap_labs.view(-1))\n",
        "            loss = loss_init + loss_final + loss_cap\n",
        "            val_loss += loss.item()\n",
        "            n_val_batches += 1\n",
        "\n",
        "            # get predictions\n",
        "            init_preds  = init_logits.argmax(dim=-1)\n",
        "            final_preds = final_logits.argmax(dim=-1)\n",
        "            cap_preds   = cap_logits.argmax(dim=-1)\n",
        "\n",
        "            # mask out padding (-100)\n",
        "            mask_init  = (init_labs.view(-1)  != -100)\n",
        "            mask_final = (final_labs.view(-1) != -100)\n",
        "            mask_cap   = (cap_labs.view(-1)   != -100)\n",
        "\n",
        "            all_init_trues.extend(init_labs.view(-1)[mask_init].cpu().tolist())\n",
        "            all_init_preds.extend(init_preds.view(-1)[mask_init].cpu().tolist())\n",
        "            all_final_trues.extend(final_labs.view(-1)[mask_final].cpu().tolist())\n",
        "            all_final_preds.extend(final_preds.view(-1)[mask_final].cpu().tolist())\n",
        "            all_cap_trues.extend(cap_labs.view(-1)[mask_cap].cpu().tolist())\n",
        "            all_cap_preds.extend(cap_preds.view(-1)[mask_cap].cpu().tolist())\n",
        "\n",
        "    avg_val_loss = val_loss / n_val_batches\n",
        "    print(f\"Epoch {epoch} — Val loss:   {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Compute macro-F1\n",
        "    f1_init_macro  = f1_score(all_init_trues,  all_init_preds,  average='macro', zero_division=0)\n",
        "    f1_final_macro = f1_score(all_final_trues, all_final_preds, average='macro', zero_division=0)\n",
        "    f1_cap_macro   = f1_score(all_cap_trues,   all_cap_preds,   average='macro', zero_division=0)\n",
        "    print(f\"Epoch {epoch} — F1 (macro): init={f1_init_macro:.3f}, final={f1_final_macro:.3f}, cap={f1_cap_macro:.3f}\")\n",
        "\n",
        "    # Per-class F1 reports\n",
        "    print(\"\\nInitial punctuation per-class F1:\")\n",
        "    print(classification_report(all_init_trues, all_init_preds, labels=[0,1], target_names=['no-¿','¿'], zero_division=0))\n",
        "\n",
        "    print(\"Final punctuation per-class F1:\")\n",
        "    print(classification_report(all_final_trues, all_final_preds,\n",
        "                                labels=[0,1,2,3],\n",
        "                                target_names=['none','.', '?', ','], zero_division=0))\n",
        "\n",
        "    print(\"Capitalization per-class F1:\")\n",
        "    print(classification_report(all_cap_trues, all_cap_preds,\n",
        "                                labels=[0,1,2,3],\n",
        "                                target_names=['lower','Initial','Mixed','ALLCAP'], zero_division=0))\n",
        "\n",
        "    print(\"-\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9kvhWTocEX"
      },
      "source": [
        "# Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzn0etHwobUx",
        "outputId": "695e0b84-d453-4371-cb29-767075ba0b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote predictions.csv\n",
            "Test set performance:\n",
            "  • Initial punctuation F1-macro: 0.8418\n",
            "  • Final punctuation   F1-macro: 0.8133\n",
            "  • Capitalization      F1-macro: 0.9016\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model.eval()\n",
        "output_rows = []\n",
        "\n",
        "# For metric accumulation\n",
        "all_init_trues,  all_init_preds  = [], []\n",
        "all_final_trues, all_final_preds = [], []\n",
        "all_cap_trues,   all_cap_preds   = [], []\n",
        "\n",
        "idx_map_init    = {0:'', 1:'¿'}\n",
        "idx_map_final   = {0:'', 1:'.', 2:'?', 3:','}\n",
        "\n",
        "for inst_id, instance in enumerate(test_data):\n",
        "    # prepare inputs\n",
        "    input_ids = torch.tensor(instance[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        init_logits, final_logits, cap_logits = model(input_ids)\n",
        "\n",
        "    # get token-level preds\n",
        "    init_pred  = init_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    final_pred = final_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    cap_pred   = cap_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "\n",
        "    # retrieve true labels\n",
        "    init_true  = instance[\"init_labels\"]\n",
        "    final_true = instance[\"final_labels\"]\n",
        "    cap_true   = instance[\"cap_labels\"]\n",
        "    tokens     = instance[\"tokens\"]\n",
        "\n",
        "    # sanity check\n",
        "    assert len(init_pred)==len(init_true)==len(tokens)\n",
        "\n",
        "    # accumulate and record\n",
        "    for token_idx, token in enumerate(tokens):\n",
        "        # append to CSV rows\n",
        "        output_rows.append({\n",
        "            \"instancia_id\": inst_id,\n",
        "            \"token_id\":     token_idx,\n",
        "            \"token\":        token,\n",
        "            \"punt_inicial\": idx_map_init[init_pred[token_idx]],\n",
        "            \"punt_final\":   idx_map_final[final_pred[token_idx]],\n",
        "            \"capitalizacion\": cap_pred[token_idx]\n",
        "        })\n",
        "        # accumulate for metrics\n",
        "        all_init_trues.append(init_true[token_idx])\n",
        "        all_init_preds.append(init_pred[token_idx])\n",
        "        all_final_trues.append(final_true[token_idx])\n",
        "        all_final_preds.append(final_pred[token_idx])\n",
        "        all_cap_trues.append(cap_true[token_idx])\n",
        "        all_cap_preds.append(cap_pred[token_idx])\n",
        "\n",
        "# build and save DataFrame\n",
        "output_df = pd.DataFrame(output_rows)\n",
        "output_df.to_csv(\"predictions.csv\", index=False)\n",
        "print(\"Wrote predictions.csv\")\n",
        "\n",
        "# compute and print macro-F1 for each task\n",
        "f1_init  = f1_score(all_init_trues,  all_init_preds,  average=\"macro\", zero_division=0)\n",
        "f1_final = f1_score(all_final_trues, all_final_preds, average=\"macro\", zero_division=0)\n",
        "f1_cap   = f1_score(all_cap_trues,   all_cap_preds,   average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"Test set performance:\")\n",
        "print(f\"  • Initial punctuation F1-macro: {f1_init:.4f}\")\n",
        "print(f\"  • Final punctuation   F1-macro: {f1_final:.4f}\")\n",
        "print(f\"  • Capitalization      F1-macro: {f1_cap:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7XNpi272lag"
      },
      "source": [
        "# Inferencia manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LtnYIyq2nKC",
        "outputId": "195d9ad3-8334-496b-f540-c1e3dad11110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:      gracias pero me voy\n",
            "Reconstructed: gracias, pero me voy\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import torch\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "\n",
        "def reconstruct_sentence(\n",
        "    raw_sentence: str,\n",
        "    model: nn.Module,\n",
        "    tokenizer: BertTokenizerFast,\n",
        "    device: torch.device,\n",
        "    idx_map_init: dict = {0:'', 1:'¿'},\n",
        "    idx_map_final: dict = {0:'', 1:'.', 2:'?', 3:','}\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Runs the model on a single raw sentence and returns the\n",
        "    reconstructed sentence with punctuation & capitalization.\n",
        "    \"\"\"\n",
        "    # 1) Tokenize (with word-ids for alignment)\n",
        "    enc = tokenizer(\n",
        "        raw_sentence.lower().split(),      # split into words so word_ids works\n",
        "        is_split_into_words=True,\n",
        "        return_offsets_mapping=False,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "    word_ids = enc.word_ids(batch_index=0)  # list of length L\n",
        "\n",
        "    # 2) Model forward\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        init_logits, final_logits, cap_logits = model(input_ids)\n",
        "    init_pred  = init_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    final_pred = final_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    cap_pred   = cap_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "\n",
        "    # 3) Gather per-word predictions\n",
        "    words: List[str] = []\n",
        "    cur_word_idx = None\n",
        "    cur_subtokens: List[str] = []\n",
        "    cur_init = ''\n",
        "    cur_cap = 0\n",
        "    cur_final = ''\n",
        "\n",
        "    for i, wid in enumerate(word_ids):\n",
        "        if wid is None:\n",
        "            continue\n",
        "        token = tokenizer.convert_ids_to_tokens(int(input_ids[0,i]))\n",
        "        # start of a new word?\n",
        "        if wid != cur_word_idx:\n",
        "            # flush previous\n",
        "            if cur_word_idx is not None:\n",
        "                # assemble the word text\n",
        "                word_text = \"\".join(cur_subtokens)\n",
        "                # apply capitalization\n",
        "                if cur_cap == 3:\n",
        "                    word_text = word_text.upper()\n",
        "                elif cur_cap == 1:\n",
        "                    word_text = word_text.capitalize()\n",
        "                elif cur_cap == 2:\n",
        "                    # we’ll uppercase the first letter only for demo\n",
        "                    word_text = word_text[0].upper() + word_text[1:]\n",
        "                # attach final punctuation\n",
        "                word_text = word_text + idx_map_final[cur_final]\n",
        "                # prepend initial punctuation if any\n",
        "                word_text = cur_init + word_text\n",
        "                words.append(word_text)\n",
        "            # reset for new word\n",
        "            cur_word_idx = wid\n",
        "            cur_subtokens = [token.replace(\"##\",\"\")]  # start fresh\n",
        "            cur_init  = idx_map_init[init_pred[i]]\n",
        "            cur_final = final_pred[i]\n",
        "            cur_cap   = cap_pred[i]\n",
        "        else:\n",
        "            # continuing same word\n",
        "            cur_subtokens.append(token.replace(\"##\",\"\"))\n",
        "            # update final & cap to last sub-token’s prediction\n",
        "            cur_final = final_pred[i]\n",
        "            # we keep init and cap from first subtoken\n",
        "    # flush last word\n",
        "    if cur_word_idx is not None:\n",
        "        word_text = \"\".join(cur_subtokens)\n",
        "        if cur_cap == 3:\n",
        "            word_text = word_text.upper()\n",
        "        elif cur_cap == 1:\n",
        "            word_text = word_text.capitalize()\n",
        "        elif cur_cap == 2:\n",
        "            word_text = word_text[0].upper() + word_text[1:]\n",
        "        word_text = word_text + idx_map_final[cur_final]\n",
        "        word_text = cur_init + word_text\n",
        "        words.append(word_text)\n",
        "\n",
        "    # finally, join with spaces:\n",
        "    return \" \".join(words)\n",
        "\n",
        "# --- Example usage ---\n",
        "raw = \"gracias pero me voy\"\n",
        "tokenizer_fast = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "recon = reconstruct_sentence(\n",
        "    raw_sentence=raw,\n",
        "    model=model,               # your trained BiLSTM or Transformer\n",
        "    tokenizer=tokenizer_fast,  # BertTokenizerFast instance\n",
        "    device=device\n",
        ")\n",
        "print(\"Input:     \", raw)\n",
        "print(\"Reconstructed:\", recon)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}