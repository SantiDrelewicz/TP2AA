{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "22Um16b8hYaa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ivanl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries (run in the notebook environment)\n",
        "\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-mvov0XojBH"
      },
      "source": [
        "# Pre procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H_cZspapu_X",
        "outputId": "1ffe82c5-d584-4853-a386-83b88d4410d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "… loaded 500000 sentences\n",
            "Done loading: 500000 sentences\n",
            "Shuffled sentences\n",
            "… processed 500000 sentences, 4096405 tokens so far\n",
            "Final: 4096405 tokens from 500000 sentences\n",
            "   inst_id  token_id  token punt_inicial punt_final  capitalizacion\n",
            "0        1         0    qui                                       1\n",
            "1        1         1  ##ero                                       1\n",
            "2        1         2  vivir                                       0\n",
            "3        2         0     es                                       1\n",
            "4        2         1    una                                       0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from typing import List, Optional\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 1) Loader function\n",
        "def read_raw_sentences(\n",
        "    path: str,\n",
        "    n_max_sentences: Optional[int] = None,\n",
        "    shuffle: bool = False,\n",
        "    random_seed: int = 0,\n",
        "    report_every: int = 100_000\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Reads up to `n_max_sentences` lines from `path` (one sentence per line).\n",
        "    Returns a list of the raw lines (with trailing newlines stripped).\n",
        "    \"\"\"\n",
        "    sentences: List[str] = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if n_max_sentences is not None:\n",
        "            for i in range(n_max_sentences):\n",
        "                line = f.readline()\n",
        "                if not line:\n",
        "                    break\n",
        "                sentences.append(line.rstrip(\"\\n\"))\n",
        "                if (i + 1) % report_every == 0:\n",
        "                    print(f\"… loaded {i+1} sentences\")\n",
        "        else:\n",
        "            for i, line in enumerate(f, start=1):\n",
        "                sentences.append(line.rstrip(\"\\n\"))\n",
        "                if i % report_every == 0:\n",
        "                    print(f\"… loaded {i} sentences\")\n",
        "    print(f\"Done loading: {len(sentences)} sentences\")\n",
        "    if shuffle:\n",
        "        random.seed(random_seed)\n",
        "        random.shuffle(sentences)\n",
        "        print(\"Shuffled sentences\")\n",
        "    return sentences\n",
        "\n",
        "# 2) Your label extractor & pattern\n",
        "pattern = re.compile(r\"\\w+|[^\\w\\s]\", flags=re.UNICODE)\n",
        "\n",
        "def extract_labels(sentence: str):\n",
        "    tokens = pattern.findall(sentence)\n",
        "    words, init_labels, final_labels, cap_labels = [], [], [], []\n",
        "    for i, token in enumerate(tokens):\n",
        "        if re.match(r\"\\w+\", token, flags=re.UNICODE):\n",
        "            # initial punctuation\n",
        "            init = '¿' if i>0 and tokens[i-1]=='¿' else ''\n",
        "            # final punctuation\n",
        "            final = tokens[i+1] if i < len(tokens)-1 and tokens[i+1] in {'.',',','?'} else ''\n",
        "            # capitalization\n",
        "            if token.isupper():\n",
        "                cap = 3\n",
        "            elif token[0].isupper() and token[1:].islower():\n",
        "                cap = 1\n",
        "            elif token.islower():\n",
        "                cap = 0\n",
        "            else:\n",
        "                cap = 2\n",
        "            words.append(token)\n",
        "            init_labels.append(init)\n",
        "            final_labels.append(final)\n",
        "            cap_labels.append(cap)\n",
        "    return words, init_labels, final_labels, cap_labels\n",
        "\n",
        "# 3) Load sentences from file\n",
        "path = \"es_419_validas.txt\"\n",
        "raw_sentences = read_raw_sentences(\n",
        "    path=path,\n",
        "    n_max_sentences=500000,   # or None to load all\n",
        "    shuffle=True,\n",
        "    random_seed=42,\n",
        "    report_every=500000\n",
        ")\n",
        "\n",
        "# 4) Tokenize+label into DataFrame\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "data = []\n",
        "for inst_id, sentence in enumerate(raw_sentences, start=1):\n",
        "    words, init_lbls, final_lbls, cap_lbls = extract_labels(sentence)\n",
        "    token_idx = 0\n",
        "    for word, init_lbl, final_lbl, cap_lbl in zip(words, init_lbls, final_lbls, cap_lbls):\n",
        "        subtokens = tokenizer.tokenize(word.lower())\n",
        "        for i, sub in enumerate(subtokens):\n",
        "            # initial only on first subtoken\n",
        "            punct_init = init_lbl if i == 0 else ''\n",
        "            # final only on last subtoken\n",
        "            punct_final = final_lbl if i == len(subtokens)-1 else ''\n",
        "            data.append([\n",
        "                inst_id,\n",
        "                token_idx,\n",
        "                sub,\n",
        "                punct_init,\n",
        "                punct_final,\n",
        "                cap_lbl\n",
        "            ])\n",
        "            token_idx += 1\n",
        "    if inst_id % 500_000 == 0:\n",
        "        print(f\"… processed {inst_id} sentences, {len(data)} tokens so far\")\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    data,\n",
        "    columns=[\"inst_id\", \"token_id\", \"token\", \"punt_inicial\", \"punt_final\", \"capitalizacion\"]\n",
        ")\n",
        "print(f\"Final: {df.shape[0]} tokens from {inst_id} sentences\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QacLF4AJoMf3",
        "outputId": "e1c6adbf-1c3f-413e-f036-37fdc12cf513"
      },
      "outputs": [],
      "source": [
        "# Convert token strings to BERT token IDs\n",
        "df[\"token_id_bert\"] = tokenizer.convert_tokens_to_ids(df[\"token\"].tolist())\n",
        "\n",
        "# Group by instance to form sequences\n",
        "grouped = {}\n",
        "for inst_id, group in df.groupby(\"inst_id\"):\n",
        "    grouped[inst_id] = {\n",
        "        \"input_ids\": group[\"token_id_bert\"].tolist(),\n",
        "        \"init_labels\": [0 if lbl=='' else 1 for lbl in group[\"punt_inicial\"]],\n",
        "        \"final_labels\": [0 if lbl=='' else (1 if lbl=='.' else (2 if lbl=='?' else 3))\n",
        "                         for lbl in group[\"punt_final\"]],\n",
        "        \"cap_labels\": group[\"capitalizacion\"].tolist(),\n",
        "        \"tokens\": group[\"token\"].tolist()\n",
        "    }\n",
        "\n",
        "# Create a list of instances for splitting\n",
        "instances = list(grouped.values())\n",
        "random.shuffle(instances)\n",
        "n = len(instances)\n",
        "train_split = int(0.8 * n)\n",
        "val_split = int(0.9 * n)\n",
        "train_data = instances[:train_split]\n",
        "val_data   = instances[train_split:val_split]\n",
        "test_data  = instances[val_split:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zlUbbNeCoO2n"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class PunctCapitalDataset(Dataset):\n",
        "    def __init__(self, instances):\n",
        "        self.instances = instances\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "    def __getitem__(self, idx):\n",
        "        inst = self.instances[idx]\n",
        "        return (\n",
        "            torch.tensor(inst[\"input_ids\"], dtype=torch.long),\n",
        "            torch.tensor(inst[\"init_labels\"], dtype=torch.long),\n",
        "            torch.tensor(inst[\"final_labels\"], dtype=torch.long),\n",
        "            torch.tensor(inst[\"cap_labels\"], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids, init_labs, final_labs, cap_labs = zip(*batch)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    init_labs  = pad_sequence(init_labs,  batch_first=True, padding_value=-100)\n",
        "    final_labs = pad_sequence(final_labs, batch_first=True, padding_value=-100)\n",
        "    cap_labs   = pad_sequence(cap_labs,   batch_first=True, padding_value=-100)\n",
        "    return input_ids, init_labs, final_labs, cap_labs\n",
        "\n",
        "train_loader = DataLoader(PunctCapitalDataset(train_data), batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(PunctCapitalDataset(val_data), batch_size=128, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7920b_Xogs_"
      },
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CL0YtW0koTiA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class JointPunctCapitalModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        hidden_dim: int,\n",
        "        num_init: int,\n",
        "        num_final: int,\n",
        "        num_cap: int,\n",
        "        n_layers: int = 1,\n",
        "        dropout: float = 0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Embedding + input dropout\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # BiLSTM with inter-layer dropout (only applies if n_layers > 1)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim // 2,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout if n_layers > 1 else 0.0,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Output dropout before heads\n",
        "        self.output_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Three classification heads\n",
        "        self.init_head  = nn.Linear(hidden_dim, num_init)\n",
        "        self.final_head = nn.Linear(hidden_dim, num_final)\n",
        "        self.cap_head   = nn.Linear(hidden_dim, num_cap)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, T]\n",
        "        emb = self.embedding(x)          # [B, T, E]\n",
        "        emb = self.input_dropout(emb)    # dropout on embeddings\n",
        "\n",
        "        out, _ = self.bilstm(emb)        # [B, T, H]\n",
        "        out = self.output_dropout(out)   # dropout on LSTM outputs\n",
        "\n",
        "        init_logits  = self.init_head(out)    # [B, T, num_init]\n",
        "        final_logits = self.final_head(out)   # [B, T, num_final]\n",
        "        cap_logits   = self.cap_head(out)     # [B, T, num_cap]\n",
        "        return init_logits, final_logits, cap_logits\n",
        "\n",
        "# Example instantiation:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = JointPunctCapitalModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_init=2,\n",
        "    num_final=4,\n",
        "    num_cap=4,\n",
        "    n_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66stgr7oeOo"
      },
      "source": [
        "# Entrenamiento y evaluacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqBqZlrQoYJv",
        "outputId": "53aa6d58-e8c6-446b-95ca-fe07b862668f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 — Train loss: 0.3600\n",
            "Epoch 1 — Val loss:   0.2517\n",
            "Epoch 1 — F1 (macro): init=0.853, final=0.817, cap=0.906\n",
            "\n",
            "Initial punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        no-¿       0.99      1.00      0.99    400149\n",
            "           ¿       0.85      0.61      0.71      9561\n",
            "\n",
            "    accuracy                           0.99    409710\n",
            "   macro avg       0.92      0.80      0.85    409710\n",
            "weighted avg       0.99      0.99      0.99    409710\n",
            "\n",
            "Final punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        none       0.98      0.98      0.98    344383\n",
            "           .       0.84      0.96      0.90     37987\n",
            "           ?       0.85      0.64      0.73      9719\n",
            "           ,       0.77      0.58      0.66     17621\n",
            "\n",
            "    accuracy                           0.95    409710\n",
            "   macro avg       0.86      0.79      0.82    409710\n",
            "weighted avg       0.95      0.95      0.95    409710\n",
            "\n",
            "Capitalization per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       lower       0.98      0.99      0.99    318947\n",
            "     Initial       0.95      0.94      0.94     86350\n",
            "       Mixed       0.94      0.72      0.81      1453\n",
            "      ALLCAP       0.96      0.81      0.88      2960\n",
            "\n",
            "    accuracy                           0.98    409710\n",
            "   macro avg       0.96      0.86      0.91    409710\n",
            "weighted avg       0.98      0.98      0.98    409710\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 2 — Train loss: 0.2422\n",
            "Epoch 2 — Val loss:   0.2259\n",
            "Epoch 2 — F1 (macro): init=0.863, final=0.834, cap=0.929\n",
            "\n",
            "Initial punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        no-¿       0.99      1.00      0.99    400149\n",
            "           ¿       0.86      0.64      0.73      9561\n",
            "\n",
            "    accuracy                           0.99    409710\n",
            "   macro avg       0.92      0.82      0.86    409710\n",
            "weighted avg       0.99      0.99      0.99    409710\n",
            "\n",
            "Final punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        none       0.98      0.98      0.98    344383\n",
            "           .       0.85      0.96      0.90     37987\n",
            "           ?       0.86      0.67      0.76      9719\n",
            "           ,       0.77      0.64      0.70     17621\n",
            "\n",
            "    accuracy                           0.96    409710\n",
            "   macro avg       0.87      0.81      0.83    409710\n",
            "weighted avg       0.96      0.96      0.96    409710\n",
            "\n",
            "Capitalization per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       lower       0.98      0.99      0.99    318947\n",
            "     Initial       0.97      0.94      0.95     86350\n",
            "       Mixed       0.96      0.80      0.87      1453\n",
            "      ALLCAP       0.96      0.85      0.90      2960\n",
            "\n",
            "    accuracy                           0.98    409710\n",
            "   macro avg       0.97      0.90      0.93    409710\n",
            "weighted avg       0.98      0.98      0.98    409710\n",
            "\n",
            "------------------------------------------------------------\n",
            "Epoch 3 — Train loss: 0.2153\n",
            "Epoch 3 — Val loss:   0.2149\n",
            "Epoch 3 — F1 (macro): init=0.872, final=0.843, cap=0.938\n",
            "\n",
            "Initial punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        no-¿       0.99      1.00      0.99    400149\n",
            "           ¿       0.84      0.68      0.75      9561\n",
            "\n",
            "    accuracy                           0.99    409710\n",
            "   macro avg       0.92      0.84      0.87    409710\n",
            "weighted avg       0.99      0.99      0.99    409710\n",
            "\n",
            "Final punctuation per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        none       0.98      0.98      0.98    344383\n",
            "           .       0.85      0.96      0.90     37987\n",
            "           ?       0.86      0.69      0.77      9719\n",
            "           ,       0.79      0.66      0.72     17621\n",
            "\n",
            "    accuracy                           0.96    409710\n",
            "   macro avg       0.87      0.82      0.84    409710\n",
            "weighted avg       0.96      0.96      0.96    409710\n",
            "\n",
            "Capitalization per-class F1:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       lower       0.99      0.99      0.99    318947\n",
            "     Initial       0.96      0.95      0.96     86350\n",
            "       Mixed       0.95      0.85      0.89      1453\n",
            "      ALLCAP       0.96      0.87      0.91      2960\n",
            "\n",
            "    accuracy                           0.98    409710\n",
            "   macro avg       0.96      0.92      0.94    409710\n",
            "weighted avg       0.98      0.98      0.98    409710\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "for epoch in range(1, 1+3):  # e.g. 5 epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "    for input_ids, init_labs, final_labs, cap_labs in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        init_labs  = init_labs.to(device)\n",
        "        final_labs = final_labs.to(device)\n",
        "        cap_labs   = cap_labs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        init_logits, final_logits, cap_logits = model(input_ids)\n",
        "\n",
        "        loss_init  = criterion(init_logits.view(-1, 2),  init_labs.view(-1))\n",
        "        loss_final = criterion(final_logits.view(-1, 4), final_labs.view(-1))\n",
        "        loss_cap   = criterion(cap_logits.view(-1, 4),   cap_labs.view(-1))\n",
        "        loss = loss_init + loss_final + loss_cap\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    avg_train_loss = running_loss / n_batches\n",
        "    print(f\"Epoch {epoch} — Train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    n_val_batches = 0\n",
        "\n",
        "    all_init_trues,  all_init_preds  = [], []\n",
        "    all_final_trues, all_final_preds = [], []\n",
        "    all_cap_trues,   all_cap_preds   = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, init_labs, final_labs, cap_labs in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            init_labs  = init_labs.to(device)\n",
        "            final_labs = final_labs.to(device)\n",
        "            cap_labs   = cap_labs.to(device)\n",
        "\n",
        "            init_logits, final_logits, cap_logits = model(input_ids)\n",
        "\n",
        "            # compute val loss\n",
        "            loss_init  = criterion(init_logits.view(-1, 2),  init_labs.view(-1))\n",
        "            loss_final = criterion(final_logits.view(-1, 4), final_labs.view(-1))\n",
        "            loss_cap   = criterion(cap_logits.view(-1, 4),   cap_labs.view(-1))\n",
        "            loss = loss_init + loss_final + loss_cap\n",
        "            val_loss += loss.item()\n",
        "            n_val_batches += 1\n",
        "\n",
        "            # get predictions\n",
        "            init_preds  = init_logits.argmax(dim=-1)\n",
        "            final_preds = final_logits.argmax(dim=-1)\n",
        "            cap_preds   = cap_logits.argmax(dim=-1)\n",
        "\n",
        "            # mask out padding (-100)\n",
        "            mask_init  = (init_labs.view(-1)  != -100)\n",
        "            mask_final = (final_labs.view(-1) != -100)\n",
        "            mask_cap   = (cap_labs.view(-1)   != -100)\n",
        "\n",
        "            all_init_trues.extend(init_labs.view(-1)[mask_init].cpu().tolist())\n",
        "            all_init_preds.extend(init_preds.view(-1)[mask_init].cpu().tolist())\n",
        "            all_final_trues.extend(final_labs.view(-1)[mask_final].cpu().tolist())\n",
        "            all_final_preds.extend(final_preds.view(-1)[mask_final].cpu().tolist())\n",
        "            all_cap_trues.extend(cap_labs.view(-1)[mask_cap].cpu().tolist())\n",
        "            all_cap_preds.extend(cap_preds.view(-1)[mask_cap].cpu().tolist())\n",
        "\n",
        "    avg_val_loss = val_loss / n_val_batches\n",
        "    print(f\"Epoch {epoch} — Val loss:   {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Compute macro-F1\n",
        "    f1_init_macro  = f1_score(all_init_trues,  all_init_preds,  average='macro', zero_division=0)\n",
        "    f1_final_macro = f1_score(all_final_trues, all_final_preds, average='macro', zero_division=0)\n",
        "    f1_cap_macro   = f1_score(all_cap_trues,   all_cap_preds,   average='macro', zero_division=0)\n",
        "    print(f\"Epoch {epoch} — F1 (macro): init={f1_init_macro:.3f}, final={f1_final_macro:.3f}, cap={f1_cap_macro:.3f}\")\n",
        "\n",
        "    # Per-class F1 reports\n",
        "    print(\"\\nInitial punctuation per-class F1:\")\n",
        "    print(classification_report(all_init_trues, all_init_preds, labels=[0,1], target_names=['no-¿','¿'], zero_division=0))\n",
        "\n",
        "    print(\"Final punctuation per-class F1:\")\n",
        "    print(classification_report(all_final_trues, all_final_preds,\n",
        "                                labels=[0,1,2,3],\n",
        "                                target_names=['none','.', '?', ','], zero_division=0))\n",
        "\n",
        "    print(\"Capitalization per-class F1:\")\n",
        "    print(classification_report(all_cap_trues, all_cap_preds,\n",
        "                                labels=[0,1,2,3],\n",
        "                                target_names=['lower','Initial','Mixed','ALLCAP'], zero_division=0))\n",
        "\n",
        "    print(\"-\"*60) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9kvhWTocEX"
      },
      "source": [
        "# Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzn0etHwobUx",
        "outputId": "695e0b84-d453-4371-cb29-767075ba0b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote predictions.csv\n",
            "Test set performance:\n",
            "  • Initial punctuation F1-macro: 0.8626\n",
            "  • Final punctuation   F1-macro: 0.8390\n",
            "  • Capitalization      F1-macro: 0.9332\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "model.eval()\n",
        "output_rows = []\n",
        "\n",
        "# For metric accumulation\n",
        "all_init_trues,  all_init_preds  = [], [] \n",
        "all_final_trues, all_final_preds = [], []\n",
        "all_cap_trues,   all_cap_preds   = [], []\n",
        "\n",
        "idx_map_init    = {0:'', 1:'¿'}\n",
        "idx_map_final   = {0:'', 1:'.', 2:'?', 3:','}\n",
        "\n",
        "for inst_id, instance in enumerate(test_data):\n",
        "    # prepare inputs\n",
        "    input_ids = torch.tensor(instance[\"input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        init_logits, final_logits, cap_logits = model(input_ids)\n",
        "\n",
        "    # get token-level preds\n",
        "    init_pred  = init_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    final_pred = final_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    cap_pred   = cap_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "\n",
        "    # retrieve true labels\n",
        "    init_true  = instance[\"init_labels\"]\n",
        "    final_true = instance[\"final_labels\"]\n",
        "    cap_true   = instance[\"cap_labels\"]\n",
        "    tokens     = instance[\"tokens\"]\n",
        "\n",
        "    # sanity check\n",
        "    assert len(init_pred)==len(init_true)==len(tokens)\n",
        "\n",
        "    # accumulate and record\n",
        "    for token_idx, token in enumerate(tokens):\n",
        "        # append to CSV rows\n",
        "        output_rows.append({\n",
        "            \"instancia_id\": inst_id,\n",
        "            \"token_id\":     token_idx,\n",
        "            \"token\":        token,\n",
        "            \"punt_inicial\": idx_map_init[init_pred[token_idx]],\n",
        "            \"punt_final\":   idx_map_final[final_pred[token_idx]],\n",
        "            \"capitalizacion\": cap_pred[token_idx]\n",
        "        })\n",
        "        # accumulate for metrics\n",
        "        all_init_trues.append(init_true[token_idx])\n",
        "        all_init_preds.append(init_pred[token_idx])\n",
        "        all_final_trues.append(final_true[token_idx])\n",
        "        all_final_preds.append(final_pred[token_idx])\n",
        "        all_cap_trues.append(cap_true[token_idx])\n",
        "        all_cap_preds.append(cap_pred[token_idx])\n",
        "\n",
        "# build and save DataFrame\n",
        "output_df = pd.DataFrame(output_rows)\n",
        "output_df.to_csv(\"predictions.csv\", index=False)\n",
        "print(\"Wrote predictions.csv\")\n",
        "\n",
        "# compute and print macro-F1 for each task\n",
        "f1_init  = f1_score(all_init_trues,  all_init_preds,  average=\"macro\", zero_division=0)\n",
        "f1_final = f1_score(all_final_trues, all_final_preds, average=\"macro\", zero_division=0)\n",
        "f1_cap   = f1_score(all_cap_trues,   all_cap_preds,   average=\"macro\", zero_division=0)\n",
        "\n",
        "print(f\"Test set performance:\")\n",
        "print(f\"  • Initial punctuation F1-macro: {f1_init:.4f}\")\n",
        "print(f\"  • Final punctuation   F1-macro: {f1_final:.4f}\")\n",
        "print(f\"  • Capitalization      F1-macro: {f1_cap:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7XNpi272lag"
      },
      "source": [
        "# Inferencia manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def reconstruct_sentence_with_tokenizer(\n",
        "    raw_sentence: str,\n",
        "    model: nn.Module,\n",
        "    tokenizer: BertTokenizer,\n",
        "    device: torch.device,\n",
        "    idx_map_init: Dict[int, str] = {0: '', 1: '¿'},\n",
        "    idx_map_final: Dict[int, str] = {0: '', 1: '.', 2: '?', 3: ','}\n",
        ") -> Tuple[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Runs the model on a single raw sentence and returns:\n",
        "    1. The reconstructed sentence (with punctuation & capitalization)\n",
        "    2. A list of predictions per word (token, init, final, cap labels)\n",
        "    Tokenization replicates the same method used in training (manual subtokens).\n",
        "    \"\"\"\n",
        "    words = raw_sentence.strip().split()\n",
        "    all_subtokens = []\n",
        "    subtoken_to_word = []\n",
        "\n",
        "    for word_idx, word in enumerate(words):\n",
        "        subtokens = tokenizer.tokenize(word.lower())\n",
        "        all_subtokens.extend(subtokens)\n",
        "        subtoken_to_word.extend([word_idx] * len(subtokens))\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(all_subtokens)\n",
        "    input_tensor = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        init_logits, final_logits, cap_logits = model(input_tensor)\n",
        "\n",
        "    init_pred  = init_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    final_pred = final_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "    cap_pred   = cap_logits.argmax(dim=-1).squeeze(0).cpu().tolist()\n",
        "\n",
        "    predictions = []\n",
        "    reconstructed_words = []\n",
        "    word_pieces = []\n",
        "    cur_word_idx = -1\n",
        "\n",
        "    for i, (subtoken, wid) in enumerate(zip(all_subtokens, subtoken_to_word)):\n",
        "        token_clean = subtoken.replace(\"##\", \"\")\n",
        "\n",
        "        if wid != cur_word_idx:\n",
        "            # flush previous word\n",
        "            if word_pieces:\n",
        "                word_str = \"\".join(word_pieces)\n",
        "                # Capitalization\n",
        "                if cur_cap == 3:\n",
        "                    word_str = word_str.upper()\n",
        "                elif cur_cap == 1:\n",
        "                    word_str = word_str.capitalize()\n",
        "                elif cur_cap == 2:\n",
        "                    word_str = word_str[0].upper() + word_str[1:]\n",
        "                # Punctuation\n",
        "                word_str = cur_init + word_str + idx_map_final[cur_final]\n",
        "                reconstructed_words.append(word_str)\n",
        "                predictions.append({\n",
        "                    \"token\": word_str,\n",
        "                    \"punt_inicial\": cur_init,\n",
        "                    \"punt_final\": idx_map_final[cur_final],\n",
        "                    \"capitalizacion\": cur_cap\n",
        "                })\n",
        "\n",
        "            # reset\n",
        "            word_pieces = [token_clean]\n",
        "            cur_word_idx = wid\n",
        "            cur_init = idx_map_init[init_pred[i]]\n",
        "            cur_final = final_pred[i]\n",
        "            cur_cap = cap_pred[i]\n",
        "        else:\n",
        "            word_pieces.append(token_clean)\n",
        "            cur_final = final_pred[i]  # update with latest subtoken prediction\n",
        "\n",
        "    # flush last word\n",
        "    if word_pieces:\n",
        "        word_str = \"\".join(word_pieces)\n",
        "        if cur_cap == 3:\n",
        "            word_str = word_str.upper()\n",
        "        elif cur_cap == 1:\n",
        "            word_str = word_str.capitalize()\n",
        "        elif cur_cap == 2:\n",
        "            word_str = word_str[0].upper() + word_str[1:]\n",
        "        word_str = cur_init + word_str + idx_map_final[cur_final]\n",
        "        reconstructed_words.append(word_str)\n",
        "        predictions.append({\n",
        "            \"token\": word_str,\n",
        "            \"punt_inicial\": cur_init,\n",
        "            \"punt_final\": idx_map_final[cur_final],\n",
        "            \"capitalizacion\": cur_cap\n",
        "        })\n",
        "\n",
        "    return \" \".join(reconstructed_words), predictions\n",
        "\n",
        "\n",
        "# Example usage\n",
        "raw = \"estara despierto\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "recon, preds = reconstruct_sentence_with_tokenizer(\n",
        "    raw_sentence=raw,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Input:\", raw)\n",
        "print(\"Reconstructed:\", recon)\n",
        "print(\"Predictions:\")\n",
        "for p in preds:\n",
        "    print(p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_weights_bidireccional.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
